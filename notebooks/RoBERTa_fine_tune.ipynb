{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Made using https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/us-patent-phrase-to-phrase-matching/'\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "df_train = pd.read_csv(data_path + train_file)\n",
    "df_train.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id           anchor                         target context\n0  4112d61851461f60         opc drum  inorganic photoconductor drum     G02\n1  09e418c93a776564  adjust gas flow              altering gas flow     F23",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(data_path + test_file)\n",
    "df_test.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  context                                              title section  class  \\\n0       A                                  HUMAN NECESSITIES       A    NaN   \n1     A01  AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...       A    1.0   \n\n  subclass  group  main_group  \n0      NaN    NaN         NaN  \n1      NaN    NaN         NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>HUMAN NECESSITIES</td>\n      <td>A</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A01</td>\n      <td>AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_codes = pd.read_csv('../data/cooperative-patent-classification-codes-meaning/titles.csv').rename(columns={\"code\" : \"context\"})\n",
    "cpc_codes.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score  \\\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50   \n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n\n                                               title  \n0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_codes = cpc_codes.rename(columns = {\"code\" : \"context\"})\n",
    "df_train = pd.merge(df_train, cpc_codes[[\"context\",\"title\"]], on =\"context\", how = \"left\")\n",
    "df_test = pd.merge(df_test, cpc_codes[[\"context\",\"title\"]], on =\"context\", how = \"left\")\n",
    "df_train.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from transformers import (PreTrainedModel, RobertaModel, RobertaTokenizerFast, RobertaConfig,\n",
    "                          get_constant_schedule_with_warmup, AdamW, RobertaTokenizer, BertTokenizerFast)\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0    (abatement | abatement of pollution, FURNITURE...\n1    (abatement | act of abating, FURNITURE; DOMEST...\nName: input, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_ANCHOR_LEN = 40\n",
    "MAX_TARGET_LEN = 50\n",
    "MAX_TITLE_LEN = 175\n",
    "\n",
    "df_train['input'] = df_train.apply(lambda x: (x.anchor + ' | ' + x.title, x.target), axis=1)  # Not sure '|' is a good idea\n",
    "df_test['input'] = df_test.apply(lambda x: (x.anchor + ' | ' + x.title, x.target), axis=1)  # Not sure '|' is a good idea\n",
    "df_train.input.iloc[:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#df_train['input_ids'] = tokenizer(df_train.input.to_list())['input_ids']\n",
    "#df_train['input_attention_mask'] = tokenizer(df_train.input.to_list())['attention_mask']\n",
    "df_train['out'] = pd.get_dummies(df_train.score, prefix='score').agg(list, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "((34649, 8), (1824, 8))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=.05, shuffle=True, random_state=41)\n",
    "df_train.shape, df_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class PatentDataset(Dataset):\n",
    "    def __init__(self, tokenizer: RobertaTokenizerFast, dataset, max_anchor_len, max_target_len, max_title_len, export=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tokenizer:\n",
    "        :param dataset:\n",
    "        :param export: This mode is designed for computing final results on a dataset that does not contain the target variable\n",
    "        \"\"\"\n",
    "        super(PatentDataset, self).__init__()\n",
    "        self.export = export\n",
    "        self.tokenizer: RobertaTokenizerFast = tokenizer\n",
    "        self.df = dataset\n",
    "        self.max_length = max_anchor_len + max_target_len + max_title_len  # FIXME\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.df.input.iloc[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            seq,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        out = dict(\n",
    "            ids=torch.tensor(ids, dtype=torch.long),\n",
    "            mask=torch.tensor(mask, dtype=torch.long),\n",
    "        )\n",
    "        if not self.export:\n",
    "            out['target'] = torch.tensor(self.df.out.iloc[index], dtype=torch.float)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "train_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_train, max_anchor_len=MAX_ANCHOR_LEN, max_target_len=MAX_TARGET_LEN, max_title_len=MAX_TITLE_LEN)\n",
    "val_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_val, max_anchor_len=MAX_ANCHOR_LEN, max_target_len=MAX_TARGET_LEN, max_title_len=MAX_TITLE_LEN)\n",
    "test_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_test, max_anchor_len=MAX_ANCHOR_LEN, max_target_len=MAX_TARGET_LEN, max_title_len=MAX_TITLE_LEN, export=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, num_workers=12)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=64, num_workers=12)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, num_workers=12, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (PreTrainedModel, RobertaModel, RobertaTokenizerFast, RobertaConfig,\n",
    "                          get_constant_schedule_with_warmup, AdamW, RobertaTokenizer, BertTokenizerFast)\n",
    "\n",
    "from transformers import BertModel\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed_everything(42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "print('Using device', device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "\n",
    "class PatentRoBERTa(pl.LightningModule):\n",
    "    # TODO add dropout\n",
    "    def __init__(self, num_classes):\n",
    "        super(PatentRoBERTa, self).__init__()\n",
    "        self.pretrained_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        _, o2 = self.pretrained_model(ids, attention_mask=mask, return_dict=False)\n",
    "        out = torch.relu(self.hidden(o2))\n",
    "        out = torch.relu(self.classifier(out))\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        loss = self._common_step(batch, batch_idx, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        ids, label, mask = self._prepare_batch(batch)\n",
    "        output = self(ids=ids, mask=mask)\n",
    "        label = label.type_as(output)\n",
    "        loss = F.cross_entropy(output, label)\n",
    "        acc = (torch.argmax(output, dim=-1) == torch.argmax(label, dim=-1)).float().mean()\n",
    "        self.log(f\"{stage}_loss\", loss, on_step=True)\n",
    "        self.log(f\"{stage}_acc\", acc, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "        ids, _, mask = self._prepare_batch(batch, include_target=False)\n",
    "        output = self(ids=ids, mask=mask)\n",
    "        return torch.argmax(output, dim=-1)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, 'test')\n",
    "\n",
    "    def _prepare_batch(self, batch, include_target=True):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        if not include_target:\n",
    "            return ids, None, mask\n",
    "        label = batch['target']\n",
    "        return ids, label, mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import logging\n",
    "from logging import WARNING\n",
    "logging.basicConfig(level=WARNING)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=2, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    gradient_clip_val=0.1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergence\n",
    "    # of the gradient for recurrent neural networks\n",
    "    auto_lr_find=True,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    "    weights_summary=\"top\",\n",
    "    max_epochs=2\n",
    ")\n",
    "\n",
    "hparams = dict(\n",
    "    num_classes=df_train.score.unique().size\n",
    ")\n",
    "\n",
    "checkpoint = None# \"lightning_logs/lightning_logs/version_6/checkpoints/epoch=4-step=5700.ckpt\"\n",
    "if checkpoint is not None:\n",
    "    model = PatentRoBERTa.load_from_checkpoint(checkpoint, **hparams)\n",
    "    print(f'Checkpoint {checkpoint} loaded')\n",
    "else:\n",
    "    model = PatentRoBERTa(**hparams)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer embeddings.word_embeddings.weight is NOT trained.\n",
      "layer embeddings.position_embeddings.weight is NOT trained.\n",
      "layer embeddings.token_type_embeddings.weight is NOT trained.\n",
      "layer embeddings.LayerNorm.weight is NOT trained.\n",
      "layer embeddings.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.0.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.0.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.0.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.0.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.0.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.0.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.1.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.1.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.1.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.1.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.1.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.1.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.2.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.2.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.2.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.2.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.2.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.2.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.3.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.3.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.3.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.3.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.3.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.3.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.4.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.4.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.4.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.4.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.4.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.4.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.5.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.5.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.5.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.5.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.5.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.5.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.6.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.6.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.6.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.6.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.6.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.6.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.7.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.7.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.7.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.7.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.7.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.7.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.8.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.8.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.8.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.8.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.8.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.8.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.9.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.9.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.9.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.9.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.9.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.9.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.10.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.10.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.10.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.10.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.10.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.10.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.11.attention.self.query.weight is trained.\n",
      "layer encoder.layer.11.attention.self.query.bias is trained.\n",
      "layer encoder.layer.11.attention.self.key.weight is trained.\n",
      "layer encoder.layer.11.attention.self.key.bias is trained.\n",
      "layer encoder.layer.11.attention.self.value.weight is trained.\n",
      "layer encoder.layer.11.attention.self.value.bias is trained.\n",
      "layer encoder.layer.11.attention.output.dense.weight is trained.\n",
      "layer encoder.layer.11.attention.output.dense.bias is trained.\n",
      "layer encoder.layer.11.attention.output.LayerNorm.weight is trained.\n",
      "layer encoder.layer.11.attention.output.LayerNorm.bias is trained.\n",
      "layer encoder.layer.11.intermediate.dense.weight is trained.\n",
      "layer encoder.layer.11.intermediate.dense.bias is trained.\n",
      "layer encoder.layer.11.output.dense.weight is trained.\n",
      "layer encoder.layer.11.output.dense.bias is trained.\n",
      "layer encoder.layer.11.output.LayerNorm.weight is trained.\n",
      "layer encoder.layer.11.output.LayerNorm.bias is trained.\n",
      "layer pooler.dense.weight is trained.\n",
      "layer pooler.dense.bias is trained.\n"
     ]
    }
   ],
   "source": [
    "deeper_layer_to_train = 11\n",
    "requires_grad = False\n",
    "\n",
    "for param in model.pretrained_model.named_parameters():\n",
    "    if 'encoder.layer.11' in param[0]:\n",
    "        requires_grad = True\n",
    "    param[1].requires_grad = requires_grad\n",
    "    print(f'layer {param[0]} is {\"NOT \" if requires_grad is False else \"\"}trained.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6607c0625a04e7da5e458805f93d241"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable training of BERT model\n",
    "# for param in model.pretrained_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\"\"\"\n",
    "requires_grad = False for all layers : val_acc_epoch = 0.3388157784938812\n",
    "\"\"\"\n",
    "\n",
    "trainer.validate(model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.validate(model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = trainer.predict(model, test_dataloader)\n",
    "\n",
    "df_test['y_pred'] = np.concatenate(results)\n",
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}