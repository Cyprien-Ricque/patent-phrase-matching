{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Made using https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/us-patent-phrase-to-phrase-matching/'\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "df_train = pd.read_csv(data_path + train_file)\n",
    "df_train.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id           anchor                         target context\n0  4112d61851461f60         opc drum  inorganic photoconductor drum     G02\n1  09e418c93a776564  adjust gas flow              altering gas flow     F23",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(data_path + test_file)\n",
    "df_test.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  context                                              title section  class  \\\n0       A                                  HUMAN NECESSITIES       A    NaN   \n1     A01  AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...       A    1.0   \n\n  subclass  group  main_group  \n0      NaN    NaN         NaN  \n1      NaN    NaN         NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>HUMAN NECESSITIES</td>\n      <td>A</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A01</td>\n      <td>AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_codes = pd.read_csv('../data/cooperative-patent-classification-codes-meaning/titles.csv').rename(columns={\"code\" : \"context\"})\n",
    "cpc_codes.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score  \\\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50   \n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n\n                                               title  \n0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_codes = cpc_codes.rename(columns = {\"code\" : \"context\"})\n",
    "df_train = pd.merge(df_train, cpc_codes[[\"context\",\"title\"]], on =\"context\", how = \"left\")\n",
    "df_test = pd.merge(df_test, cpc_codes[[\"context\",\"title\"]], on =\"context\", how = \"left\")\n",
    "df_train.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from transformers import (PreTrainedModel, RobertaModel, RobertaTokenizerFast, RobertaConfig,\n",
    "                          get_constant_schedule_with_warmup, AdamW, RobertaTokenizer, BertTokenizerFast)\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0    (abatement | FURNITURE; DOMESTIC ARTICLES OR A...\n1    (abatement | FURNITURE; DOMESTIC ARTICLES OR A...\nName: input, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_ANCHOR_LEN = 40\n",
    "MAX_TARGET_LEN = 50\n",
    "MAX_TITLE_LEN = 175\n",
    "\n",
    "df_train['input'] = df_train.apply(lambda x: (x.anchor + ' | ' + x.title, x.target), axis=1)  # Not sure '|' is a good idea\n",
    "df_test['input'] = df_test.apply(lambda x: (x.anchor + ' | ' + x.title, x.target), axis=1)  # Not sure '|' is a good idea\n",
    "df_train.input.iloc[:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#df_train['input_ids'] = tokenizer(df_train.input.to_list())['input_ids']\n",
    "#df_train['input_attention_mask'] = tokenizer(df_train.input.to_list())['attention_mask']\n",
    "df_train['out'] = pd.get_dummies(df_train.score, prefix='score').agg(list, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "((34649, 8), (1824, 8))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=.05, shuffle=True, random_state=41)\n",
    "df_train.shape, df_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class PatentDataset(Dataset):\n",
    "    def __init__(self, tokenizer: RobertaTokenizerFast, dataset, max_anchor_len, max_target_len, max_title_len, export=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tokenizer:\n",
    "        :param dataset:\n",
    "        :param export: This mode is designed for computing final results on a dataset that does not contain the target variable\n",
    "        \"\"\"\n",
    "        super(PatentDataset, self).__init__()\n",
    "        self.export = export\n",
    "        self.tokenizer: RobertaTokenizerFast = tokenizer\n",
    "        self.df = dataset\n",
    "        self.max_length = max_anchor_len + max_target_len + max_title_len  # FIXME\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.df.input.iloc[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            seq,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        out = dict(\n",
    "            ids=torch.tensor(ids, dtype=torch.long),\n",
    "            mask=torch.tensor(mask, dtype=torch.long),\n",
    "        )\n",
    "        if not self.export:\n",
    "            out['target'] = torch.tensor(self.df.out.iloc[index], dtype=torch.float)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "train_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_train, max_anchor_len=MAX_ANCHOR_LEN, max_target_len=MAX_TARGET_LEN, max_title_len=MAX_TITLE_LEN)\n",
    "val_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_val, max_anchor_len=MAX_ANCHOR_LEN, max_target_len=MAX_TARGET_LEN, max_title_len=MAX_TITLE_LEN)\n",
    "test_dataset = PatentDataset(tokenizer=tokenizer, dataset=df_test, max_anchor_len=MAX_ANCHOR_LEN, max_target_len=MAX_TARGET_LEN, max_title_len=MAX_TITLE_LEN, export=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, num_workers=12)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=64, num_workers=12)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, num_workers=12, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (PreTrainedModel, RobertaModel, RobertaTokenizerFast, RobertaConfig,\n",
    "                          get_constant_schedule_with_warmup, AdamW, RobertaTokenizer, BertTokenizerFast)\n",
    "\n",
    "from transformers import BertModel\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed_everything(42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "print('Using device', device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "\n",
    "class PatentRoBERTa(pl.LightningModule):\n",
    "    # TODO add dropout\n",
    "    def __init__(self, num_classes):\n",
    "        super(PatentRoBERTa, self).__init__()\n",
    "        self.pretrained_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        _, o2 = self.pretrained_model(ids, attention_mask=mask, return_dict=False)\n",
    "        out = torch.relu(self.hidden(o2))\n",
    "        out = torch.relu(self.classifier(out))\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        loss = self._common_step(batch, batch_idx, 'train')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        ids, label, mask = self._prepare_batch(batch)\n",
    "        output = self(ids=ids, mask=mask)\n",
    "        label = label.type_as(output)\n",
    "        loss = F.cross_entropy(output, label)\n",
    "        acc = (torch.argmax(output, dim=-1) == torch.argmax(label, dim=-1)).float().mean()\n",
    "        self.log(f\"{stage}_loss\", loss, on_step=True)\n",
    "        self.log(f\"{stage}_acc\", acc, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "        ids, _, mask = self._prepare_batch(batch, include_target=False)\n",
    "        output = self(ids=ids, mask=mask)\n",
    "        return torch.argmax(output, dim=-1)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, batch_idx, 'test')\n",
    "\n",
    "    def _prepare_batch(self, batch, include_target=True):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        if not include_target:\n",
    "            return ids, None, mask\n",
    "        label = batch['target']\n",
    "        return ids, label, mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import logging\n",
    "from logging import WARNING\n",
    "logging.basicConfig(level=WARNING)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=2, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    gradient_clip_val=0.1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergence\n",
    "    # of the gradient for recurrent neural networks\n",
    "    auto_lr_find=True,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    "    weights_summary=\"top\",\n",
    "    max_epochs=2\n",
    ")\n",
    "\n",
    "hparams = dict(\n",
    "    num_classes=df_train.score.unique().size\n",
    ")\n",
    "\n",
    "checkpoint = None# \"lightning_logs/lightning_logs/version_6/checkpoints/epoch=4-step=5700.ckpt\"\n",
    "if checkpoint is not None:\n",
    "    model = PatentRoBERTa.load_from_checkpoint(checkpoint, **hparams)\n",
    "    print(f'Checkpoint {checkpoint} loaded')\n",
    "else:\n",
    "    model = PatentRoBERTa(**hparams)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer embeddings.word_embeddings.weight is NOT trained.\n",
      "layer embeddings.position_embeddings.weight is NOT trained.\n",
      "layer embeddings.token_type_embeddings.weight is NOT trained.\n",
      "layer embeddings.LayerNorm.weight is NOT trained.\n",
      "layer embeddings.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.0.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.0.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.0.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.0.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.0.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.0.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.0.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.0.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.1.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.1.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.1.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.1.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.1.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.1.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.1.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.1.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.2.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.2.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.2.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.2.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.2.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.2.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.2.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.2.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.3.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.3.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.3.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.3.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.3.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.3.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.3.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.3.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.4.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.4.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.4.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.4.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.4.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.4.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.4.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.4.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.5.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.5.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.5.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.5.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.5.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.5.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.5.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.5.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.6.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.6.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.6.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.6.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.6.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.6.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.6.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.6.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.7.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.7.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.7.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.7.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.7.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.7.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.7.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.7.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.8.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.8.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.8.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.8.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.8.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.8.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.8.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.8.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.9.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.9.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.9.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.9.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.9.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.9.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.9.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.9.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.self.query.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.self.query.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.self.key.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.self.key.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.self.value.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.self.value.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.10.attention.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.10.attention.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.10.intermediate.dense.weight is NOT trained.\n",
      "layer encoder.layer.10.intermediate.dense.bias is NOT trained.\n",
      "layer encoder.layer.10.output.dense.weight is NOT trained.\n",
      "layer encoder.layer.10.output.dense.bias is NOT trained.\n",
      "layer encoder.layer.10.output.LayerNorm.weight is NOT trained.\n",
      "layer encoder.layer.10.output.LayerNorm.bias is NOT trained.\n",
      "layer encoder.layer.11.attention.self.query.weight is trained.\n",
      "layer encoder.layer.11.attention.self.query.bias is trained.\n",
      "layer encoder.layer.11.attention.self.key.weight is trained.\n",
      "layer encoder.layer.11.attention.self.key.bias is trained.\n",
      "layer encoder.layer.11.attention.self.value.weight is trained.\n",
      "layer encoder.layer.11.attention.self.value.bias is trained.\n",
      "layer encoder.layer.11.attention.output.dense.weight is trained.\n",
      "layer encoder.layer.11.attention.output.dense.bias is trained.\n",
      "layer encoder.layer.11.attention.output.LayerNorm.weight is trained.\n",
      "layer encoder.layer.11.attention.output.LayerNorm.bias is trained.\n",
      "layer encoder.layer.11.intermediate.dense.weight is trained.\n",
      "layer encoder.layer.11.intermediate.dense.bias is trained.\n",
      "layer encoder.layer.11.output.dense.weight is trained.\n",
      "layer encoder.layer.11.output.dense.bias is trained.\n",
      "layer encoder.layer.11.output.LayerNorm.weight is trained.\n",
      "layer encoder.layer.11.output.LayerNorm.bias is trained.\n",
      "layer pooler.dense.weight is trained.\n",
      "layer pooler.dense.bias is trained.\n"
     ]
    }
   ],
   "source": [
    "deeper_layer_to_train = 11\n",
    "requires_grad = False\n",
    "\n",
    "for param in model.pretrained_model.named_parameters():\n",
    "    if 'encoder.layer.11' in param[0]:\n",
    "        requires_grad = True\n",
    "    param[1].requires_grad = requires_grad\n",
    "    print(f'layer {param[0]} is {\"NOT \" if requires_grad is False else \"\"}trained.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4771e4427b954986adda2d74769434f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_acc_epoch          0.211074560880661\n",
      "     val_loss_epoch         1.6103986501693726\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'val_loss_epoch': 1.6103986501693726, 'val_acc_epoch': 0.211074560880661}]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable training of BERT model\n",
    "# for param in model.pretrained_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\"\"\"\n",
    "requires_grad = False for all layers : val_acc_epoch = 0.3388157784938812\n",
    "\"\"\"\n",
    "\n",
    "trainer.validate(model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type         | Params\n",
      "--------------------------------------------------\n",
      "0 | pretrained_model | RobertaModel | 124 M \n",
      "1 | hidden           | Linear       | 590 K \n",
      "2 | classifier       | Linear       | 3.8 K \n",
      "--------------------------------------------------\n",
      "8.3 M     Trainable params\n",
      "116 M     Non-trainable params\n",
      "125 M     Total params\n",
      "500.960   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1af0eaaf95f142d1b78514f2048d2270"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80d7e763a632411bb59ca798afa8e607"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54f7d044e1fe481e8fcd453ade3b9f25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce0c269a0bdb42f0befd435e39c9bca4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "617ef27e83704e79936ed44e16492926"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_acc_epoch         0.20888157188892365\n",
      "     val_loss_epoch          1.609437346458435\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'val_loss_epoch': 1.609437346458435, 'val_acc_epoch': 0.20888157188892365}]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 542it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51faea890434440683230a86cf6a6655"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                  id                       anchor  \\\n0   4112d61851461f60                     opc drum   \n1   09e418c93a776564              adjust gas flow   \n2   36baf228038e314b               lower trunnion   \n3   1f37ead645e7f0c8                cap component   \n4   71a5b6ad068d531f           neural stimulation   \n5   474c874d0c07bd21                     dry corn   \n6   442c114ed5c4e3c9          tunneling capacitor   \n7   b8ae62ea5e1d8bdb      angular contact bearing   \n8   faaddaf8fcba8a3f  produce liquid hydrocarbons   \n9   ae0262c02566d2ce             diesel fuel tank   \n10  a8808e31641e856d            chemical activity   \n11  16ae4b99d3601e60         transmit to platform   \n12  25c555ca3d5a2092                  oil tankers   \n13  5203a36c501f1b7c            generate in layer   \n14  b9fdc772bb8fd61c                 slip segment   \n15  7aa5908a77a7ec24                   el display   \n16  d19ef3979396d47e              overflow device   \n17  fd83613b7843f5e1     beam traveling direction   \n18  2a619016908bfa45                   el display   \n19  733979d75f59770d               equipment unit   \n20  6546846df17f9800                   halocarbyl   \n21  3ff0e7a35015be69         perfluoroalkyl group   \n22  12ca31f018a2e2b9          speed control means   \n23  03ba802ed4029e4d                   arm design   \n24  c404f8b378cbb008               hybrid bearing   \n25  78243984c02a72e4                     end pins   \n26  de51114bc0faec3e             organic starting   \n27  7e3aff857f056bf9                make of slabs   \n28  26c3c6dc6174b589                   seal teeth   \n29  b892011ab2e2cabc            carry by platform   \n30  8247ff562ca185cc                        polls   \n31  c057aecbba832387              upper clamp arm   \n32  9f2279ce667b21dc              clocked storage   \n33  b9ea2b06a878df6f              coupling factor   \n34  79795133c30ef097       different conductivity   \n35  25522ee5411e63e9               hybrid bearing   \n\n                           target context  \\\n0   inorganic photoconductor drum     G02   \n1               altering gas flow     F23   \n2                  lower locating     B60   \n3                   upper portion     D06   \n4       artificial neural network     H04   \n5                 dry corn starch     C12   \n6               capacitor housing     G11   \n7       contact therapy radiation     B23   \n8        produce a treated stream     C10   \n9               diesel fuel tanks     F02   \n10     dielectric characteristics     B01   \n11               direct receiving     H04   \n12                   oil carriers     B63   \n13              generate by layer     G02   \n14                   slip portion     B22   \n15                   illumination     G02   \n16                     oil filler     E04   \n17                  concrete beam     H05   \n18             electroluminescent     C23   \n19                power detection     H02   \n20      halogen addition reaction     C07   \n21                        hydroxy     A63   \n22                   control loop     G05   \n23                    steel plate     F16   \n24                 bearing system     F04   \n25                       end days     A44   \n26                organic farming     B61   \n27                    making cake     E04   \n28                teeth whitening     F01   \n29              carry on platform     B60   \n30                 pooling device     B21   \n31                     end visual     A61   \n32         clocked storage device     G01   \n33                turns impedance     G01   \n34               carrier polarity     H03   \n35            corrosion resistant     F16   \n\n                                                title  \\\n0                                              OPTICS   \n1          COMBUSTION APPARATUS; COMBUSTION PROCESSES   \n2                                 VEHICLES IN GENERAL   \n3   TREATMENT OF TEXTILES OR THE LIKE; LAUNDERING;...   \n4                    ELECTRIC COMMUNICATION TECHNIQUE   \n5   BIOCHEMISTRY; BEER; SPIRITS; WINE; VINEGAR; MI...   \n6                                 INFORMATION STORAGE   \n7   MACHINE TOOLS; METAL-WORKING NOT OTHERWISE PRO...   \n8   PETROLEUM, GAS OR COKE INDUSTRIES; TECHNICAL G...   \n9   COMBUSTION ENGINES; HOT-GAS OR COMBUSTION-PROD...   \n10  PHYSICAL OR CHEMICAL PROCESSES OR APPARATUS IN...   \n11                   ELECTRIC COMMUNICATION TECHNIQUE   \n12  SHIPS OR OTHER WATERBORNE VESSELS; RELATED EQU...   \n13                                             OPTICS   \n14                         CASTING; POWDER METALLURGY   \n15                                             OPTICS   \n16                                           BUILDING   \n17     ELECTRIC TECHNIQUES NOT OTHERWISE PROVIDED FOR   \n18  COATING METALLIC MATERIAL; COATING MATERIAL WI...   \n19  GENERATION; CONVERSION OR DISTRIBUTION OF ELEC...   \n20                                  ORGANIC CHEMISTRY   \n21                          SPORTS; GAMES; AMUSEMENTS   \n22                            CONTROLLING; REGULATING   \n23  ENGINEERING ELEMENTS AND UNITS; GENERAL MEASUR...   \n24  POSITIVE - DISPLACEMENT MACHINES FOR LIQUIDS; ...   \n25                            HABERDASHERY; JEWELLERY   \n26                                           RAILWAYS   \n27                                           BUILDING   \n28  MACHINES OR ENGINES IN GENERAL; ENGINE PLANTS ...   \n29                                VEHICLES IN GENERAL   \n30  MECHANICAL METAL-WORKING WITHOUT ESSENTIALLY R...   \n31             MEDICAL OR VETERINARY SCIENCE; HYGIENE   \n32                                 MEASURING; TESTING   \n33                                 MEASURING; TESTING   \n34                         BASIC ELECTRONIC CIRCUITRY   \n35  ENGINEERING ELEMENTS AND UNITS; GENERAL MEASUR...   \n\n                                                input  y_pred  \n0   (opc drum | OPTICS, inorganic photoconductor d...       0  \n1   (adjust gas flow | COMBUSTION APPARATUS; COMBU...       0  \n2   (lower trunnion | VEHICLES IN GENERAL, lower l...       0  \n3   (cap component | TREATMENT OF TEXTILES OR THE ...       0  \n4   (neural stimulation | ELECTRIC COMMUNICATION T...       0  \n5   (dry corn | BIOCHEMISTRY; BEER; SPIRITS; WINE;...       0  \n6   (tunneling capacitor | INFORMATION STORAGE, ca...       0  \n7   (angular contact bearing | MACHINE TOOLS; META...       0  \n8   (produce liquid hydrocarbons | PETROLEUM, GAS ...       0  \n9   (diesel fuel tank | COMBUSTION ENGINES; HOT-GA...       0  \n10  (chemical activity | PHYSICAL OR CHEMICAL PROC...       0  \n11  (transmit to platform | ELECTRIC COMMUNICATION...       0  \n12  (oil tankers | SHIPS OR OTHER WATERBORNE VESSE...       0  \n13    (generate in layer | OPTICS, generate by layer)       0  \n14  (slip segment | CASTING; POWDER METALLURGY, sl...       0  \n15                (el display | OPTICS, illumination)       0  \n16           (overflow device | BUILDING, oil filler)       0  \n17  (beam traveling direction | ELECTRIC TECHNIQUE...       0  \n18  (el display | COATING METALLIC MATERIAL; COATI...       0  \n19  (equipment unit | GENERATION; CONVERSION OR DI...       0  \n20  (halocarbyl | ORGANIC CHEMISTRY, halogen addit...       0  \n21  (perfluoroalkyl group | SPORTS; GAMES; AMUSEME...       0  \n22  (speed control means | CONTROLLING; REGULATING...       0  \n23  (arm design | ENGINEERING ELEMENTS AND UNITS; ...       0  \n24  (hybrid bearing | POSITIVE - DISPLACEMENT MACH...       0  \n25     (end pins | HABERDASHERY; JEWELLERY, end days)       0  \n26     (organic starting | RAILWAYS, organic farming)       0  \n27            (make of slabs | BUILDING, making cake)       0  \n28  (seal teeth | MACHINES OR ENGINES IN GENERAL; ...       0  \n29  (carry by platform | VEHICLES IN GENERAL, carr...       0  \n30  (polls | MECHANICAL METAL-WORKING WITHOUT ESSE...       0  \n31  (upper clamp arm | MEDICAL OR VETERINARY SCIEN...       0  \n32  (clocked storage | MEASURING; TESTING, clocked...       0  \n33  (coupling factor | MEASURING; TESTING, turns i...       0  \n34  (different conductivity | BASIC ELECTRONIC CIR...       0  \n35  (hybrid bearing | ENGINEERING ELEMENTS AND UNI...       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>title</th>\n      <th>input</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n      <td>OPTICS</td>\n      <td>(opc drum | OPTICS, inorganic photoconductor d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n      <td>COMBUSTION APPARATUS; COMBUSTION PROCESSES</td>\n      <td>(adjust gas flow | COMBUSTION APPARATUS; COMBU...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36baf228038e314b</td>\n      <td>lower trunnion</td>\n      <td>lower locating</td>\n      <td>B60</td>\n      <td>VEHICLES IN GENERAL</td>\n      <td>(lower trunnion | VEHICLES IN GENERAL, lower l...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1f37ead645e7f0c8</td>\n      <td>cap component</td>\n      <td>upper portion</td>\n      <td>D06</td>\n      <td>TREATMENT OF TEXTILES OR THE LIKE; LAUNDERING;...</td>\n      <td>(cap component | TREATMENT OF TEXTILES OR THE ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71a5b6ad068d531f</td>\n      <td>neural stimulation</td>\n      <td>artificial neural network</td>\n      <td>H04</td>\n      <td>ELECTRIC COMMUNICATION TECHNIQUE</td>\n      <td>(neural stimulation | ELECTRIC COMMUNICATION T...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>474c874d0c07bd21</td>\n      <td>dry corn</td>\n      <td>dry corn starch</td>\n      <td>C12</td>\n      <td>BIOCHEMISTRY; BEER; SPIRITS; WINE; VINEGAR; MI...</td>\n      <td>(dry corn | BIOCHEMISTRY; BEER; SPIRITS; WINE;...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>442c114ed5c4e3c9</td>\n      <td>tunneling capacitor</td>\n      <td>capacitor housing</td>\n      <td>G11</td>\n      <td>INFORMATION STORAGE</td>\n      <td>(tunneling capacitor | INFORMATION STORAGE, ca...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>b8ae62ea5e1d8bdb</td>\n      <td>angular contact bearing</td>\n      <td>contact therapy radiation</td>\n      <td>B23</td>\n      <td>MACHINE TOOLS; METAL-WORKING NOT OTHERWISE PRO...</td>\n      <td>(angular contact bearing | MACHINE TOOLS; META...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>faaddaf8fcba8a3f</td>\n      <td>produce liquid hydrocarbons</td>\n      <td>produce a treated stream</td>\n      <td>C10</td>\n      <td>PETROLEUM, GAS OR COKE INDUSTRIES; TECHNICAL G...</td>\n      <td>(produce liquid hydrocarbons | PETROLEUM, GAS ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ae0262c02566d2ce</td>\n      <td>diesel fuel tank</td>\n      <td>diesel fuel tanks</td>\n      <td>F02</td>\n      <td>COMBUSTION ENGINES; HOT-GAS OR COMBUSTION-PROD...</td>\n      <td>(diesel fuel tank | COMBUSTION ENGINES; HOT-GA...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>a8808e31641e856d</td>\n      <td>chemical activity</td>\n      <td>dielectric characteristics</td>\n      <td>B01</td>\n      <td>PHYSICAL OR CHEMICAL PROCESSES OR APPARATUS IN...</td>\n      <td>(chemical activity | PHYSICAL OR CHEMICAL PROC...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>16ae4b99d3601e60</td>\n      <td>transmit to platform</td>\n      <td>direct receiving</td>\n      <td>H04</td>\n      <td>ELECTRIC COMMUNICATION TECHNIQUE</td>\n      <td>(transmit to platform | ELECTRIC COMMUNICATION...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>25c555ca3d5a2092</td>\n      <td>oil tankers</td>\n      <td>oil carriers</td>\n      <td>B63</td>\n      <td>SHIPS OR OTHER WATERBORNE VESSELS; RELATED EQU...</td>\n      <td>(oil tankers | SHIPS OR OTHER WATERBORNE VESSE...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5203a36c501f1b7c</td>\n      <td>generate in layer</td>\n      <td>generate by layer</td>\n      <td>G02</td>\n      <td>OPTICS</td>\n      <td>(generate in layer | OPTICS, generate by layer)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>b9fdc772bb8fd61c</td>\n      <td>slip segment</td>\n      <td>slip portion</td>\n      <td>B22</td>\n      <td>CASTING; POWDER METALLURGY</td>\n      <td>(slip segment | CASTING; POWDER METALLURGY, sl...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7aa5908a77a7ec24</td>\n      <td>el display</td>\n      <td>illumination</td>\n      <td>G02</td>\n      <td>OPTICS</td>\n      <td>(el display | OPTICS, illumination)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>d19ef3979396d47e</td>\n      <td>overflow device</td>\n      <td>oil filler</td>\n      <td>E04</td>\n      <td>BUILDING</td>\n      <td>(overflow device | BUILDING, oil filler)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>fd83613b7843f5e1</td>\n      <td>beam traveling direction</td>\n      <td>concrete beam</td>\n      <td>H05</td>\n      <td>ELECTRIC TECHNIQUES NOT OTHERWISE PROVIDED FOR</td>\n      <td>(beam traveling direction | ELECTRIC TECHNIQUE...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2a619016908bfa45</td>\n      <td>el display</td>\n      <td>electroluminescent</td>\n      <td>C23</td>\n      <td>COATING METALLIC MATERIAL; COATING MATERIAL WI...</td>\n      <td>(el display | COATING METALLIC MATERIAL; COATI...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>733979d75f59770d</td>\n      <td>equipment unit</td>\n      <td>power detection</td>\n      <td>H02</td>\n      <td>GENERATION; CONVERSION OR DISTRIBUTION OF ELEC...</td>\n      <td>(equipment unit | GENERATION; CONVERSION OR DI...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6546846df17f9800</td>\n      <td>halocarbyl</td>\n      <td>halogen addition reaction</td>\n      <td>C07</td>\n      <td>ORGANIC CHEMISTRY</td>\n      <td>(halocarbyl | ORGANIC CHEMISTRY, halogen addit...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>3ff0e7a35015be69</td>\n      <td>perfluoroalkyl group</td>\n      <td>hydroxy</td>\n      <td>A63</td>\n      <td>SPORTS; GAMES; AMUSEMENTS</td>\n      <td>(perfluoroalkyl group | SPORTS; GAMES; AMUSEME...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>12ca31f018a2e2b9</td>\n      <td>speed control means</td>\n      <td>control loop</td>\n      <td>G05</td>\n      <td>CONTROLLING; REGULATING</td>\n      <td>(speed control means | CONTROLLING; REGULATING...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>03ba802ed4029e4d</td>\n      <td>arm design</td>\n      <td>steel plate</td>\n      <td>F16</td>\n      <td>ENGINEERING ELEMENTS AND UNITS; GENERAL MEASUR...</td>\n      <td>(arm design | ENGINEERING ELEMENTS AND UNITS; ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>c404f8b378cbb008</td>\n      <td>hybrid bearing</td>\n      <td>bearing system</td>\n      <td>F04</td>\n      <td>POSITIVE - DISPLACEMENT MACHINES FOR LIQUIDS; ...</td>\n      <td>(hybrid bearing | POSITIVE - DISPLACEMENT MACH...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>78243984c02a72e4</td>\n      <td>end pins</td>\n      <td>end days</td>\n      <td>A44</td>\n      <td>HABERDASHERY; JEWELLERY</td>\n      <td>(end pins | HABERDASHERY; JEWELLERY, end days)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>de51114bc0faec3e</td>\n      <td>organic starting</td>\n      <td>organic farming</td>\n      <td>B61</td>\n      <td>RAILWAYS</td>\n      <td>(organic starting | RAILWAYS, organic farming)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7e3aff857f056bf9</td>\n      <td>make of slabs</td>\n      <td>making cake</td>\n      <td>E04</td>\n      <td>BUILDING</td>\n      <td>(make of slabs | BUILDING, making cake)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>26c3c6dc6174b589</td>\n      <td>seal teeth</td>\n      <td>teeth whitening</td>\n      <td>F01</td>\n      <td>MACHINES OR ENGINES IN GENERAL; ENGINE PLANTS ...</td>\n      <td>(seal teeth | MACHINES OR ENGINES IN GENERAL; ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>b892011ab2e2cabc</td>\n      <td>carry by platform</td>\n      <td>carry on platform</td>\n      <td>B60</td>\n      <td>VEHICLES IN GENERAL</td>\n      <td>(carry by platform | VEHICLES IN GENERAL, carr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>8247ff562ca185cc</td>\n      <td>polls</td>\n      <td>pooling device</td>\n      <td>B21</td>\n      <td>MECHANICAL METAL-WORKING WITHOUT ESSENTIALLY R...</td>\n      <td>(polls | MECHANICAL METAL-WORKING WITHOUT ESSE...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>c057aecbba832387</td>\n      <td>upper clamp arm</td>\n      <td>end visual</td>\n      <td>A61</td>\n      <td>MEDICAL OR VETERINARY SCIENCE; HYGIENE</td>\n      <td>(upper clamp arm | MEDICAL OR VETERINARY SCIEN...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>9f2279ce667b21dc</td>\n      <td>clocked storage</td>\n      <td>clocked storage device</td>\n      <td>G01</td>\n      <td>MEASURING; TESTING</td>\n      <td>(clocked storage | MEASURING; TESTING, clocked...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>b9ea2b06a878df6f</td>\n      <td>coupling factor</td>\n      <td>turns impedance</td>\n      <td>G01</td>\n      <td>MEASURING; TESTING</td>\n      <td>(coupling factor | MEASURING; TESTING, turns i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>79795133c30ef097</td>\n      <td>different conductivity</td>\n      <td>carrier polarity</td>\n      <td>H03</td>\n      <td>BASIC ELECTRONIC CIRCUITRY</td>\n      <td>(different conductivity | BASIC ELECTRONIC CIR...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>25522ee5411e63e9</td>\n      <td>hybrid bearing</td>\n      <td>corrosion resistant</td>\n      <td>F16</td>\n      <td>ENGINEERING ELEMENTS AND UNITS; GENERAL MEASUR...</td>\n      <td>(hybrid bearing | ENGINEERING ELEMENTS AND UNI...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.predict(model, test_dataloader)\n",
    "\n",
    "df_test['y_pred'] = np.concatenate(results)\n",
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}