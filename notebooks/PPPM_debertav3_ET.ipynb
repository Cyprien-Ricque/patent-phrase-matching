{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263c913e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Directory settings"
   ],
   "id": "263c913e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_hXhg3jjjeoT",
    "outputId": "4a77671e-5159-465e-91e7-b42f7419a175",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install wandb --user\n",
    "# !pip3 install pytorch_lightning --user\n",
    "# !pip3 install tokenizers --user\n",
    "# !pip3 install transformers --user\n",
    "# !pip3 install sentencepiece --user"
   ],
   "id": "_hXhg3jjjeoT"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "46d8c76e",
    "outputId": "f45ed1b1-550e-41f7-d5cd-97424e0dee28",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "running_on = 'local'\n",
    "\n",
    "INPUT_DIR = '../data/' # '/content/drive/MyDrive/data/patent_data/' # '../data/'  # ../input/\n",
    "OUTPUT_DIR = './'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# !ls /content/drive/MyDrive/data/patent_data"
   ],
   "id": "46d8c76e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c75f6918",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CFG"
   ],
   "id": "c75f6918"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3905c1b1-e822-448b-830a-3011c17fe836",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**To change for non debug run**\n",
    "\n",
    "- deeper_layer_to_train (0)\n",
    "- model path (../input/deberta-v3-large/deberta-v3-large (for kaggle))\n",
    "- batch size (16)\n",
    "- epoch (4)\n",
    "- num_workers (2 or 4)\n",
    "- waidb_logger (True)"
   ],
   "id": "3905c1b1-e822-448b-830a-3011c17fe836"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "80d58afe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='PPPM'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    num_workers=2\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=4\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=4\n",
    "    fc_dropout=0.2\n",
    "    target_size=1\n",
    "    max_len=133\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=4\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    deeper_layer_to_train=17\n",
    "    waidb_logger=False\n",
    "    val_size=0.1\n",
    "\n",
    "dict_config = {a: CFG.__dict__[a] for a in CFG.__dict__ if a[:2] != '__'}"
   ],
   "id": "80d58afe"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "B    8019\nH    6195\nG    6013\nC    5288\nA    4094\nF    4054\nE    1531\nD    1279\nName: sector, dtype: int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sector.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ed638be",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Library"
   ],
   "id": "5ed638be"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ebee5bbe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.11.0\n",
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.20.1\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ],
   "id": "ebee5bbe"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcc0106a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Utils"
   ],
   "id": "dcc0106a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "540df385",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "    return score\n",
    "\n",
    "seed_everything(seed=CFG.seed)"
   ],
   "id": "540df385"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32170fde",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Loading"
   ],
   "id": "32170fde"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a903a50f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (36473, 5)\n",
      "test.shape: (36, 4)\n",
      "submission.shape: (36, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                 id           anchor                         target context\n0  4112d61851461f60         opc drum  inorganic photoconductor drum     G02\n1  09e418c93a776564  adjust gas flow              altering gas flow     F23",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                 id  score\n0  4112d61851461f60      0\n1  09e418c93a776564      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(INPUT_DIR + 'us-patent-phrase-to-phrase-matching/train.csv')\n",
    "test = pd.read_csv(INPUT_DIR + 'us-patent-phrase-to-phrase-matching/test.csv')\n",
    "submission = pd.read_csv(INPUT_DIR + 'us-patent-phrase-to-phrase-matching/sample_submission.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "print(f\"submission.shape: {submission.shape}\")\n",
    "\n",
    "display(train.head(2))\n",
    "display(test.head(2))\n",
    "display(submission.head(2))"
   ],
   "id": "a903a50f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fa85ac77",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score                                       context_text\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>context_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                 id           anchor                         target context                                       context_text\n0  4112d61851461f60         opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS\n1  09e418c93a776564  adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>context_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n      <td>PHYSICS. OPTICS</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CPC Data\n",
    "# ====================================================\n",
    "def get_cpc_texts():\n",
    "    contexts = []\n",
    "    pattern = '[A-Z]\\d+'\n",
    "    for file_name in os.listdir(INPUT_DIR + '/cpc-data/CPCSchemeXML202105'):\n",
    "        result = re.findall(pattern, file_name)\n",
    "        if result:\n",
    "            contexts.append(result)\n",
    "    contexts = sorted(set(sum(contexts, [])))\n",
    "    results = {}\n",
    "    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "        with open(f'{INPUT_DIR}/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n",
    "            s = f.read()\n",
    "        pattern = f'{cpc}\\t\\t.+'\n",
    "        result = re.findall(pattern, s)\n",
    "        cpc_result = result[0].lstrip(pattern)\n",
    "        for context in [c for c in contexts if c[0] == cpc]:\n",
    "            pattern = f'{context}\\t\\t.+'\n",
    "            result = re.findall(pattern, s)\n",
    "            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n",
    "    return results\n",
    "\n",
    "\n",
    "cpc_texts = get_cpc_texts()\n",
    "torch.save(cpc_texts, OUTPUT_DIR + \"cpc_texts.pth\")\n",
    "train['context_text'] = train['context'].map(cpc_texts)\n",
    "test['context_text'] = test['context'].map(cpc_texts)\n",
    "display(train.head(2))\n",
    "display(test.head(2))"
   ],
   "id": "fa85ac77"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7bb73eb6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                 id     anchor                  target context  score                                       context_text                                               text sector\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]abatement of pollution[SEP]HUMAN...      A\n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]act of abating[SEP]HUMAN NECESSI...      A",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>context_text</th>\n      <th>text</th>\n      <th>sector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[SEP]abatement of pollution[SEP]HUMAN...</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n      <td>abatement[SEP]act of abating[SEP]HUMAN NECESSI...</td>\n      <td>A</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                 id           anchor                         target context                                       context_text                                               text sector\n0  4112d61851461f60         opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS  opc drum[SEP]inorganic photoconductor drum[SEP...      G\n1  09e418c93a776564  adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...  adjust gas flow[SEP]altering gas flow[SEP]MECH...      F",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>context_text</th>\n      <th>text</th>\n      <th>sector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n      <td>PHYSICS. OPTICS</td>\n      <td>opc drum[SEP]inorganic photoconductor drum[SEP...</td>\n      <td>G</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n      <td>adjust gas flow[SEP]altering gas flow[SEP]MECH...</td>\n      <td>F</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\n",
    "test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n",
    "train['sector'] = train.context.str[0]\n",
    "test['sector'] = test.context.str[0]\n",
    "display(train.head(2))\n",
    "display(test.head(2))"
   ],
   "id": "7bb73eb6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "63a192e1-9404-473a-8445-215ba835a2b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train = train.iloc[:1000]  # DEBUG"
   ],
   "id": "63a192e1-9404-473a-8445-215ba835a2b9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d4cb9ba9-0d99-4fd0-8aa1-b1eca851d196",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((32825, 8), (3648, 8), (36, 7))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(train, test_size=CFG.val_size, shuffle=True, random_state=CFG.seed)\n",
    "df_test = test\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ],
   "id": "d4cb9ba9-0d99-4fd0-8aa1-b1eca851d196"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7adea75",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenizer"
   ],
   "id": "a7adea75"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "54f8891e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "CFG.tokenizer = tokenizer"
   ],
   "id": "54f8891e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9fca227",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset"
   ],
   "id": "d9fca227"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "49be5a92",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer(text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df, input_col='text', label_col='score'):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df[input_col].values\n",
    "        self.labels = df[label_col].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n"
   ],
   "id": "49be5a92"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44d28ba3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ],
   "id": "44d28ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4fda903b-9f2b-438f-9971-af23dfe93d59",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(pl.LightningModule):\n",
    "    def __init__(self, cfg, num_train_steps, fold:int = -1):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model_cfg = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config=self.model_cfg)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.model_cfg.hidden_size, cfg.target_size)\n",
    "        self._init_weights(self.fc)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.model_cfg.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self._init_weights(self.attention)\n",
    "        self.configure_trained_layers(self.cfg.deeper_layer_to_train)\n",
    "        self.all_t = np.array([])\n",
    "        self.all_p = np.array([])\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        self.str_fold = f'fold({fold})/' if fold != -1 else ''\n",
    "        self.fold = fold\n",
    "        \n",
    "    def configure_trained_layers(self, deeper_layer_to_train, verbose=0):\n",
    "        if deeper_layer_to_train == -1:\n",
    "          return\n",
    "        requires_grad = False\n",
    "        print(f'deeper_layer_to_train: {deeper_layer_to_train}')\n",
    "        for param in self.model.named_parameters():\n",
    "            if f'encoder.layer.{deeper_layer_to_train}' in param[0]:\n",
    "                requires_grad = True\n",
    "            param[1].requires_grad = requires_grad\n",
    "            if verbose == 2 or (verbose == 1 and requires_grad):\n",
    "                print(f'layer {param[0]} is {\"NOT \" if requires_grad is False else \"\"}trained.')\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_cfg.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_cfg.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_parameters = get_optimizer_params(self, encoder_lr=self.cfg.encoder_lr, decoder_lr=self.cfg.decoder_lr, weight_decay=self.cfg.weight_decay)\n",
    "        optimizer = AdamW(optimizer_parameters, lr=self.cfg.encoder_lr, eps=self.cfg.eps, betas=self.cfg.betas)\n",
    "    \n",
    "        scheduler = get_scheduler(self.cfg, optimizer, self.num_train_steps)\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        d = self._common_step(batch, batch_idx, 'train')\n",
    "        return d['loss']\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        inputs, labels = batch\n",
    "        y_hat = self(inputs)\n",
    "\n",
    "        loss = self.criterion(y_hat.view(-1, 1), labels.view(-1, 1))\n",
    "\n",
    "        labels = labels.view(-1).detach().cpu()\n",
    "        y_hat = y_hat.view(-1).detach().cpu()\n",
    "        if stage == 'train':\n",
    "            self.all_t = np.concatenate([self.all_t, labels.numpy()])\n",
    "            self.all_p = np.concatenate([self.all_p, y_hat.numpy()])\n",
    "        \n",
    "        pearson_batch = 1000\n",
    "        if len(self.all_t) > pearson_batch and stage == 'train':\n",
    "            self.log(f\"{self.str_fold}Pearson_batch{pearson_batch}/{stage}\", get_score(self.all_t, self.all_p), on_step=True, prog_bar=True)\n",
    "            self.all_t = np.array([])\n",
    "            self.all_p = np.array([])\n",
    "        \n",
    "        self._log_metrics(loss, labels, y_hat, stage)\n",
    "        return {'loss': loss, 'y_true': labels, 'y_pred': y_hat}\n",
    "\n",
    "    def _log_metrics(self, loss, labels, y_hat, stage: str) -> None:\n",
    "        y_true = torch.round(labels * 4).to(int).cpu()\n",
    "        y_hat_cat = torch.min(torch.max(torch.round(y_hat.to(torch.float) * 4).to(int), torch.tensor(0)), torch.tensor(4)).cpu()\n",
    "        # Compute metrics\n",
    "        acc = (y_true == y_hat_cat).float().mean()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(f\"{self.str_fold}Loss/{stage}\", loss, on_step=True, prog_bar=stage == 'train')\n",
    "        if self.fold != -1:\n",
    "            self.log(f\"Loss/{stage}\", loss)\n",
    "        self.log(f\"{self.str_fold}Accuracy/{stage}\", acc, on_step=True, prog_bar=stage == 'train')\n",
    "        self.log(f\"fold\", self.fold)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "        inputs, labels = batch\n",
    "        y_hat = self(inputs)\n",
    "        return y_hat.view(-1)\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        y_hat = torch.concat([t['y_pred'] for t in val_step_outputs])\n",
    "        y_true = torch.concat([t['y_true'] for t in val_step_outputs])\n",
    "        score = get_score(y_true, y_hat)\n",
    "        print('score', score)\n",
    "        self.log('Pearson/val', score)"
   ],
   "id": "4fda903b-9f2b-438f-9971-af23dfe93d59"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "582f3c42-d216-47d5-be2d-eefa35deef8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# ====================================================\n",
    "# Callbacks\n",
    "# ====================================================\n",
    "lr_logger = LearningRateMonitor(logging_interval='step')\n",
    "tb_logger = TensorBoardLogger(save_dir=\"./logs/\", name=\"Deberta_v3_large_finetune_CV\")\n",
    "loggers = [tb_logger]\n",
    "\n",
    "if CFG.waidb_logger:\n",
    "    wb_logger = WandbLogger(\n",
    "        name=f\"Deberta_v3_large_finetune_CV\",  save_dir='./logs', offline=False, project='PPPM',\n",
    "        notes=\"Deberta_v3_large_finetune\", tags=[\"deberta_fine_tune\", CFG.model], config=dict_config\n",
    "    )\n",
    "    loggers += [wb_logger]\n",
    "\n",
    "best_models = {}    \n",
    "\n",
    "def fit_models():\n",
    "    for sector in df_train.sector.unique():\n",
    "        df_train_sector = df_train[df_train.sector == sector]\n",
    "        df_val_sector = df_val[df_val.sector == sector]\n",
    "        print('Sector', sector, df_train_sector.shape, df_val_sector.shape)\n",
    "\n",
    "        # ====================================================\n",
    "        # Training\n",
    "        # ====================================================\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath='logs', filename='model_'+str(sector)+'-{epoch}', verbose=True, save_top_k=1, mode='max', monitor='Pearson/val',\n",
    "            auto_insert_metric_name=True\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator='gpu', gradient_clip_val=CFG.max_grad_norm, weights_summary=\"top\", max_epochs=CFG.epochs,\n",
    "            callbacks=[lr_logger, checkpoint_callback], logger=loggers,  accumulate_grad_batches=CFG.gradient_accumulation_steps,\n",
    "            precision=16, amp_backend=\"native\", gpus=1\n",
    "        )\n",
    "\n",
    "        # ====================================================\n",
    "        # Data Loaders\n",
    "        # ====================================================\n",
    "        train_dataset = TrainDataset(CFG, df_train_sector)\n",
    "        valid_dataset = TrainDataset(CFG, df_val_sector)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True, drop_last=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False\n",
    "        )\n",
    "        hparams = dict(cfg=CFG, num_train_steps=int(len(train_loader) * CFG.epochs), fold=-1)\n",
    "        model = CustomModel(**hparams)\n",
    "\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "        print(f\"BEST MODEL PATH\", checkpoint_callback.best_model_path)\n",
    "\n",
    "        # Get best model\n",
    "        model = CustomModel.load_from_checkpoint(checkpoint_callback.best_model_path, **hparams)\n",
    "\n",
    "        trainer.validate(model, valid_loader)\n",
    "        # Predict\n",
    "        y_hat = trainer.predict(model, valid_loader)\n",
    "        df_val.loc[(df_val.sector == sector), 'pred'] = np.concatenate(y_hat)\n",
    "        best_models[sector] = checkpoint_callback.best_model_path\n",
    "\n",
    "    del model, trainer, train_dataset, valid_dataset, train_loader, valid_loader, checkpoint_callback\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# fit_models()\n",
    "#\n",
    "# with open('./logs/best_models.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_models, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "id": "582f3c42-d216-47d5-be2d-eefa35deef8c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "65e88cf9-550e-470f-8f8a-c411ba46ee26",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(f'Spearman corr', get_score(df_val.score, df_val.pred))"
   ],
   "id": "65e88cf9-550e-470f-8f8a-c411ba46ee26"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6b47157-3f47-4383-8a31-4a8e42195b1f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prediction"
   ],
   "id": "b6b47157-3f47-4383-8a31-4a8e42195b1f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_B-epoch=2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "793c9e36b89d486d8148397ed2a0c6ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_C-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef8435404fe8491e91eaab953f4fd57b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_E-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d78c3cfaec4a4fca9ffbfc08af50b77c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_G-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9136cdd88cbb48598447d601c4926a19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_H-epoch=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e35211c4ee4e4565a26068ee8f91f235"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_F-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c1de494420a4be4b4ff5e1d72d9d030"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_D-epoch=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d454cc266b0a409cabb92d16cede93b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_A-epoch=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b081d3b01d0452fa67a18a46f609e16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "nan"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision=16,\n",
    "    amp_backend=\"native\",\n",
    "    gpus=1\n",
    ")\n",
    "\n",
    "df_test['score'] = np.nan\n",
    "df_test['pred'] = 0\n",
    "\n",
    "with open('./logs/deberta_ET/best_models.pkl', 'rb') as f:\n",
    "    best_models = pickle.load(f)\n",
    "\n",
    "df_val['pred_enhanced_1'] = 0\n",
    "df_val['pred_enhanced_2'] = 0\n",
    "df_val['pred_enhanced_3'] = 0\n",
    "df_val['pred_enhanced_4'] = 0\n",
    "df_val['pred_enhanced_5'] = 0\n",
    "\n",
    "for sector in best_models:\n",
    "    hparams = dict(cfg=CFG, num_train_steps=None, fold=-1)\n",
    "\n",
    "    test_dataset = TrainDataset(CFG, df_val)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=42,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    checkpoint = best_models[sector].replace('logs/', 'logs/deberta_ET/')\n",
    "    print(\"checkpoint\", checkpoint)\n",
    "\n",
    "    model = CustomModel.load_from_checkpoint(checkpoint, **hparams)\n",
    "    y_hat = predictor.predict(model, test_loader)\n",
    "\n",
    "    # Basic\n",
    "    df_val['pred_enhanced_1'] += np.concatenate(y_hat) / len(best_models)\n",
    "\n",
    "    # More weight to main\n",
    "    df_val['pred_enhanced_2'] += np.concatenate(y_hat) / len(best_models)\n",
    "    df_val.loc[df_val.sector == sector, 'pred_enhanced_2'] += np.concatenate(y_hat)[df_val.sector == sector] / len(best_models)\n",
    "\n",
    "    # Even more\n",
    "    df_val['pred_enhanced_3'] += np.concatenate(y_hat) / len(best_models)\n",
    "    df_val.loc[df_val.sector == sector, 'pred_enhanced_3'] += np.concatenate(y_hat)[df_val.sector == sector] / len(best_models)\n",
    "    df_val.loc[df_val.sector == sector, 'pred_enhanced_3'] += np.concatenate(y_hat)[df_val.sector == sector] / len(best_models)\n",
    "\n",
    "    # Weighted\n",
    "    df_val['pred_enhanced_4'] += np.concatenate(y_hat) / len(best_models) * (df_train.sector == sector).sum()\n",
    "\n",
    "    # Weighted + weight to main\n",
    "    df_val['pred_enhanced_5'] += np.concatenate(y_hat) / len(best_models) * (df_train.sector == sector).sum()\n",
    "    df_val.loc[df_val.sector == sector, 'pred_enhanced_5'] += np.concatenate(y_hat)[df_val.sector == sector] / len(best_models) * (df_train.sector == sector).sum()\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_val[['pred_enhanced']].hist(bins=40)\n",
    "\n",
    "get_score(df_val.score, df_val.pred_enhanced) # Basic 0.784070  # advanced 0.7900615"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.7840694079060868,\n 0.7900615264344355,\n 0.7916054385718889,\n 0.781004755589928,\n 0.7821407095626574)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(df_val.score, df_val.pred_enhanced_1), get_score(df_val.score, df_val.pred_enhanced_2), get_score(df_val.score, df_val.pred_enhanced_3), get_score(df_val.score, df_val.pred_enhanced_4), get_score(df_val.score, df_val.pred_enhanced_5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'B': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_B-epoch=2.ckpt',\n 'C': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_C-epoch=0.ckpt',\n 'E': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_E-epoch=0.ckpt',\n 'G': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_G-epoch=0.ckpt',\n 'H': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_H-epoch=1.ckpt',\n 'F': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_F-epoch=0.ckpt',\n 'D': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_D-epoch=1.ckpt',\n 'A': '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_A-epoch=1.ckpt'}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_models = {'B': '../input/pppm-checkpoints-deberta-et/model_B-epoch=2.ckpt',\n",
    "'C': '../input/pppm-checkpoints-deberta-et/model_C-epoch=0.ckpt',\n",
    "'E': '../input/pppm-checkpoints-deberta-et/model_E-epoch=0.ckpt',\n",
    "'G': '../input/pppm-checkpoints-deberta-et/model_G-epoch=0.ckpt',\n",
    "'H': '../input/pppm-checkpoints-deberta-et/model_H-epoch=1.ckpt',\n",
    "'F': '../input/pppm-checkpoints-deberta-et/model_F-epoch=0.ckpt',\n",
    "'D': '../input/pppm-checkpoints-deberta-et/model_D-epoch=1.ckpt',\n",
    "'A': '../input/pppm-checkpoints-deberta-et/model_A-epoch=1.ckpt'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_B-epoch=2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6941c16e71e4359afa82b52bb17847e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_C-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55d2f8df21b6491280060732729218bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_E-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "794881f175d44fc6a4218a915783754f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_G-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba1d8e57350c42219c4e57d2c55f98cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_H-epoch=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "689b60aa0140497c8491dc65143df5e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_F-epoch=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72680b91b8d345d8b4129f16b5b0e37c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_D-epoch=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6c0e69e72824338a4b103f1957c2bd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/deberta_ET/model_A-epoch=1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeper_layer_to_train: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8da8f1a4a0646c48ffb24737b0c97ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0.74184014015749"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision=16,\n",
    "    amp_backend=\"native\",\n",
    "    gpus=1\n",
    ")\n",
    "\n",
    "df_test['score'] = np.nan\n",
    "df_test['pred'] = 0\n",
    "\n",
    "with open('./logs/deberta_ET/best_models.pkl', 'rb') as f:\n",
    "    best_models = pickle.load(f)\n",
    "\n",
    "for sector in best_models:\n",
    "    hparams = dict(cfg=CFG, num_train_steps=None, fold=-1)\n",
    "\n",
    "    test_dataset = TrainDataset(CFG, df_val[df_val.sector == sector])\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    checkpoint = best_models[sector].replace('logs/', 'logs/deberta_ET/')\n",
    "    print(\"checkpoint\", checkpoint)\n",
    "\n",
    "    model = CustomModel.load_from_checkpoint(checkpoint, **hparams)\n",
    "    y_hat = predictor.predict(model, test_loader)\n",
    "\n",
    "    df_val.loc[df_val.sector == sector, 'pred'] = np.concatenate(y_hat)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "get_score(df_val.score, df_val.pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint /home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_B-epoch=2.ckpt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_B-epoch=2.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     27\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m best_models[sector]\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m, checkpoint)\n\u001B[0;32m---> 30\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mCustomModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m predictor\u001B[38;5;241m.\u001B[39mpredict(model, test_loader)\n\u001B[1;32m     33\u001B[0m df_test\u001B[38;5;241m.\u001B[39mloc[df_test\u001B[38;5;241m.\u001B[39msector \u001B[38;5;241m==\u001B[39m sector, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpred\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(y_hat)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:139\u001B[0m, in \u001B[0;36mModelIO.load_from_checkpoint\u001B[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m         checkpoint \u001B[38;5;241m=\u001B[39m pl_load(checkpoint_path, map_location\u001B[38;5;241m=\u001B[39mmap_location)\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 139\u001B[0m         checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mpl_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloc\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hparams_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    142\u001B[0m     extension \u001B[38;5;241m=\u001B[39m hparams_file\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:46\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path_or_url, map_location)\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mhub\u001B[38;5;241m.\u001B[39mload_state_dict_from_url(\u001B[38;5;28mstr\u001B[39m(path_or_url), map_location\u001B[38;5;241m=\u001B[39mmap_location)\n\u001B[1;32m     45\u001B[0m fs \u001B[38;5;241m=\u001B[39m get_filesystem(path_or_url)\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mload(f, map_location\u001B[38;5;241m=\u001B[39mmap_location)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/spec.py:1037\u001B[0m, in \u001B[0;36mAbstractFileSystem.open\u001B[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001B[0m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1036\u001B[0m     ac \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautocommit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_intrans)\n\u001B[0;32m-> 1037\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1038\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1039\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[43m        \u001B[49m\u001B[43mblock_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1041\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautocommit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mac\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1043\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1044\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1045\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1046\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfsspec\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompression\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compr\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/implementations/local.py:159\u001B[0m, in \u001B[0;36mLocalFileSystem._open\u001B[0;34m(self, path, mode, block_size, **kwargs)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_mkdir \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent(path), exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 159\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mLocalFileOpener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/implementations/local.py:254\u001B[0m, in \u001B[0;36mLocalFileOpener.__init__\u001B[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression \u001B[38;5;241m=\u001B[39m get_compression(path, compression)\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocksize \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE\n\u001B[0;32m--> 254\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/fsspec/implementations/local.py:259\u001B[0m, in \u001B[0;36mLocalFileOpener._open\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mclosed:\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocommit \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m--> 259\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression:\n\u001B[1;32m    261\u001B[0m             compress \u001B[38;5;241m=\u001B[39m compr[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression]\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/cyprien/Documents/github/patent-phrase-matching/notebooks/logs/model_B-epoch=2.ckpt'"
     ]
    }
   ],
   "source": [
    "predictor = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision=16,\n",
    "    amp_backend=\"native\",\n",
    "    gpus=1\n",
    ")\n",
    "\n",
    "df_test['score'] = np.nan\n",
    "df_test['pred'] = 0\n",
    "\n",
    "with open('./logs/deberta_ET/best_models.pkl', 'rb') as f:\n",
    "    best_models = pickle.load(f)\n",
    "\n",
    "for sector in best_models:\n",
    "    hparams = dict(cfg=CFG, num_train_steps=None, fold=-1)\n",
    "\n",
    "    test_dataset = TrainDataset(CFG, df_test[df_test.sector == sector])\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    checkpoint = best_models[sector]\n",
    "    print(\"checkpoint\", checkpoint)\n",
    "\n",
    "    model = CustomModel.load_from_checkpoint(checkpoint, **hparams)\n",
    "    y_hat = predictor.predict(model, test_loader)\n",
    "\n",
    "    df_test.loc[df_test.sector == sector, 'pred'] = np.concatenate(y_hat)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_test[['pred']].hist(bins=40)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "db75cb4c-e276-4715-8dfd-64d2ba6734d3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission['score'] = df_test['pred']\n",
    "display(submission.head())\n",
    "submission[['id', 'score']].to_csv('submission.csv', index=False)"
   ],
   "id": "db75cb4c-e276-4715-8dfd-64d2ba6734d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a7a41e09-9705-40fc-8b23-a73164dec290",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.waidb_logger:\n",
    "    wandb.finish()"
   ],
   "id": "a7a41e09-9705-40fc-8b23-a73164dec290"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PPPM_debertav3_EL.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30329.017674,
   "end_time": "2022-03-22T18:05:22.040486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-22T09:39:53.022812",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}