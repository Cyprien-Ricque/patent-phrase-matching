{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263c913e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Directory settings"
   ],
   "id": "263c913e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install wandb --user\n",
    "!pip3 install pytorch_lightning --user\n",
    "!pip3 install tokenizers --user\n",
    "!pip3 install transformers --user \n",
    "!pip3 install sentencepiece --user"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "running_on = 'local'\n",
    "\n",
    "INPUT_DIR = '/content/drive/MyDrive/data/patent_data/' # '../data/'  # ../input/\n",
    "OUTPUT_DIR = '/content/'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "!ls /content/drive/MyDrive/data/patent_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CFG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**To change for non debug run**\n",
    "\n",
    "- deeper_layer_to_train (0)\n",
    "- model path (../input/deberta-v3-large/deberta-v3-large (for kaggle))\n",
    "- batch size (16)\n",
    "- epoch (4)\n",
    "- num_workers (2 or 4)\n",
    "- waidb_logger (True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='PPPM'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=4\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=16\n",
    "    fc_dropout=0.2\n",
    "    target_size=1\n",
    "    max_len=133\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    deeper_layer_to_train=-1\n",
    "    waidb_logger=False\n",
    "    val_size=0.1\n",
    "\n",
    "dict_config = {a: CFG.__dict__[a] for a in CFG.__dict__ if a[:2] != '__'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Library"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "    return score\n",
    "\n",
    "seed_everything(seed=CFG.seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv(INPUT_DIR + 'us-patent-phrase-to-phrase-matching/train.csv')\n",
    "test = pd.read_csv(INPUT_DIR + 'us-patent-phrase-to-phrase-matching/test.csv')\n",
    "submission = pd.read_csv(INPUT_DIR + 'us-patent-phrase-to-phrase-matching/sample_submission.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "print(f\"submission.shape: {submission.shape}\")\n",
    "\n",
    "display(train.head(2))\n",
    "display(test.head(2))\n",
    "display(submission.head(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CPC Data\n",
    "# ====================================================\n",
    "def get_cpc_texts():\n",
    "    contexts = []\n",
    "    pattern = '[A-Z]\\d+'\n",
    "    for file_name in os.listdir(INPUT_DIR + '/cpc-data/CPCSchemeXML202105'):\n",
    "        result = re.findall(pattern, file_name)\n",
    "        if result:\n",
    "            contexts.append(result)\n",
    "    contexts = sorted(set(sum(contexts, [])))\n",
    "    results = {}\n",
    "    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "        with open(f'{INPUT_DIR}/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n",
    "            s = f.read()\n",
    "        pattern = f'{cpc}\\t\\t.+'\n",
    "        result = re.findall(pattern, s)\n",
    "        cpc_result = result[0].lstrip(pattern)\n",
    "        for context in [c for c in contexts if c[0] == cpc]:\n",
    "            pattern = f'{context}\\t\\t.+'\n",
    "            result = re.findall(pattern, s)\n",
    "            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n",
    "    return results\n",
    "\n",
    "\n",
    "cpc_texts = get_cpc_texts()\n",
    "torch.save(cpc_texts, OUTPUT_DIR + \"cpc_texts.pth\")\n",
    "train['context_text'] = train['context'].map(cpc_texts)\n",
    "test['context_text'] = test['context'].map(cpc_texts)\n",
    "display(train.head(2))\n",
    "display(test.head(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\n",
    "test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n",
    "display(train.head(2))\n",
    "display(test.head(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = train.iloc[:100]  # DEBUG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(train, test_size=CFG.val_size, shuffle=False, random_state=CFG.seed)\n",
    "df_test = test\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "CFG.tokenizer = tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer(text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df, input_col='text', label_col='score'):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df[input_col].values\n",
    "        self.labels = df[label_col].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(pl.LightningModule):\n",
    "    def __init__(self, cfg, num_train_steps, fold:int = -1):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model_cfg = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config=self.model_cfg)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.model_cfg.hidden_size, cfg.target_size)\n",
    "        self._init_weights(self.fc)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.model_cfg.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self._init_weights(self.attention)\n",
    "        self.configure_trained_layers(self.cfg.deeper_layer_to_train)\n",
    "        self.all_t = np.array([])\n",
    "        self.all_p = np.array([])\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        self.str_fold = f'fold({fold})/' if fold != -1 else ''\n",
    "        self.fold = fold\n",
    "        \n",
    "    def configure_trained_layers(self, deeper_layer_to_train, verbose=0):\n",
    "        if deeper_layer_to_train == -1:\n",
    "          return\n",
    "        requires_grad = False\n",
    "        print(f'deeper_layer_to_train: {deeper_layer_to_train}')\n",
    "        for param in self.model.named_parameters():\n",
    "            if f'encoder.layer.{deeper_layer_to_train}' in param[0]:\n",
    "                requires_grad = True\n",
    "            param[1].requires_grad = requires_grad\n",
    "            if verbose == 2 or (verbose == 1 and requires_grad):\n",
    "                print(f'layer {param[0]} is {\"NOT \" if requires_grad is False else \"\"}trained.')\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_cfg.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_cfg.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_parameters = get_optimizer_params(self, encoder_lr=self.cfg.encoder_lr, decoder_lr=self.cfg.decoder_lr, weight_decay=self.cfg.weight_decay)\n",
    "        optimizer = AdamW(optimizer_parameters, lr=self.cfg.encoder_lr, eps=self.cfg.eps, betas=self.cfg.betas)\n",
    "    \n",
    "        scheduler = get_scheduler(self.cfg, optimizer, self.num_train_steps)\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        d = self._common_step(batch, batch_idx, 'train')\n",
    "        return d['loss']\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._common_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage: str):\n",
    "        inputs, labels = batch\n",
    "        y_hat = self(inputs)\n",
    "\n",
    "        loss = self.criterion(y_hat.view(-1, 1), labels.view(-1, 1))\n",
    "\n",
    "        labels = labels.view(-1).detach().cpu()\n",
    "        y_hat = y_hat.view(-1).detach().cpu()\n",
    "        if stage == 'train':\n",
    "            self.all_t = np.concatenate([self.all_t, labels.numpy()])\n",
    "            self.all_p = np.concatenate([self.all_p, y_hat.numpy()])\n",
    "        \n",
    "        pearson_batch = 1000\n",
    "        if len(self.all_t) > pearson_batch and stage == 'train':\n",
    "            self.log(f\"{self.str_fold}Pearson_batch{pearson_batch}/{stage}\", get_score(self.all_t, self.all_p), on_step=True, prog_bar=True)\n",
    "            self.all_t = np.array([])\n",
    "            self.all_p = np.array([])\n",
    "        \n",
    "        self._log_metrics(loss, labels, y_hat, stage)\n",
    "        return {'loss': loss, 'y_true': labels, 'y_pred': y_hat}\n",
    "\n",
    "    def _log_metrics(self, loss, labels, y_hat, stage: str) -> None:\n",
    "        y_true = torch.round(labels * 4).to(int).cpu()\n",
    "        y_hat_cat = torch.min(torch.max(torch.round(y_hat.to(torch.float) * 4).to(int), torch.tensor(0)), torch.tensor(4)).cpu()\n",
    "        # Compute metrics\n",
    "        acc = (y_true == y_hat_cat).float().mean()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(f\"{self.str_fold}Loss/{stage}\", loss, on_step=True, prog_bar=stage == 'train')\n",
    "        if self.fold != -1:\n",
    "            self.log(f\"Loss/{stage}\", loss)\n",
    "        self.log(f\"{self.str_fold}Accuracy/{stage}\", acc, on_step=True, prog_bar=stage == 'train')\n",
    "        self.log(f\"fold\", self.fold)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "        inputs, labels = batch\n",
    "        y_hat = self(inputs)\n",
    "        return y_hat.view(-1)\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        y_hat = torch.concat([t['y_pred'] for t in val_step_outputs])\n",
    "        y_true = torch.concat([t['y_true'] for t in val_step_outputs])\n",
    "        score = get_score(y_true, y_hat)\n",
    "        print('score', score)\n",
    "        self.log('Pearson/val', score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Callbacks\n",
    "# ====================================================\n",
    "lr_logger = LearningRateMonitor(logging_interval='step')\n",
    "tb_logger = TensorBoardLogger(save_dir=\"./logs/\", name=\"Deberta_v3_large_finetune_CV\")\n",
    "loggers = [tb_logger]\n",
    "\n",
    "if CFG.waidb_logger:\n",
    "    wb_logger = WandbLogger(\n",
    "        name=f\"Deberta_v3_large_finetune_CV\",  save_dir='./logs', offline=False, project='PPPM',\n",
    "        notes=\"Deberta_v3_large_finetune\", tags=[\"deberta_fine_tune\", CFG.model], config=dict_config\n",
    "    )\n",
    "    loggers += [wb_logger]\n",
    "\n",
    "best_models = {}    \n",
    "\n",
    "def fit_submodel():\n",
    "    for score_target in df_train.score.unique():\n",
    "        print('score_target', score_target)\n",
    "        # Create binary target\n",
    "        df_train['score_target'] = (df_train.score == score_target).astype(float)\n",
    "        df_val['score_target'] = (df_val.score == score_target).astype(float)\n",
    "        df_train_fold = df_train\n",
    "        df_val_fold = df_val\n",
    "\n",
    "        # ====================================================\n",
    "        # Training\n",
    "        # ====================================================\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath='logs', filename='model_'+str(score_target)+'-{epoch}', verbose=True, save_top_k=1, mode='min', monitor='Loss/val'\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            \n",
    "            enable_checkpointing=False,\n",
    "\n",
    "            accelerator='gpu', gradient_clip_val=CFG.max_grad_norm, weights_summary=\"top\", max_epochs=CFG.epochs,\n",
    "            callbacks=[lr_logger, checkpoint_callback], logger=loggers,  accumulate_grad_batches=CFG.gradient_accumulation_steps,\n",
    "            precision=16, amp_backend=\"native\", gpus=1\n",
    "        )\n",
    "\n",
    "        # ====================================================\n",
    "        # Data Loaders\n",
    "        # ====================================================\n",
    "        train_dataset = TrainDataset(CFG, df_train_fold, label_col='score_target')\n",
    "        valid_dataset = TrainDataset(CFG, df_val_fold, label_col='score_target')\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True, drop_last=True\n",
    "        )\n",
    "        train_loader_pred = DataLoader(\n",
    "            train_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False\n",
    "        )\n",
    "        hparams = dict(cfg=CFG, num_train_steps=int(len(train_loader) * CFG.epochs), fold=-1)\n",
    "        model = CustomModel(**hparams)\n",
    "\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "        print(f\"BEST MODEL PATH\", checkpoint_callback.best_model_path)\n",
    "\n",
    "        # Get best model\n",
    "\n",
    "        # del model\n",
    "        # model = CustomModel.load_from_checkpoint(checkpoint_callback.best_model_path, **hparams)\n",
    "\n",
    "        trainer.validate(model, valid_loader)\n",
    "        # Predict\n",
    "        y_hat = trainer.predict(model, valid_loader)\n",
    "        df_val[f'pred_{score_target}'] = np.concatenate(y_hat)\n",
    "\n",
    "        y_hat = trainer.predict(model, train_loader_pred)\n",
    "        df_train[f'pred_{score_target}'] = np.concatenate(y_hat)\n",
    "\n",
    "        best_models[str(score_target)] = checkpoint_callback.best_model_path\n",
    "\n",
    "\n",
    "def fit_final_model(fold):\n",
    "    df_train_fold = df_train\n",
    "    df_val_fold = df_val\n",
    "\n",
    "    # ====================================================\n",
    "    # Training\n",
    "    # ====================================================\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='logs', filename='model-{epoch}', verbose=True, save_top_k=1, mode='max', monitor='Pearson/val'\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu', gradient_clip_val=CFG.max_grad_norm, weights_summary=\"top\", max_epochs=CFG.epochs,\n",
    "        callbacks=[lr_logger, checkpoint_callback], logger=loggers,  accumulate_grad_batches=CFG.gradient_accumulation_steps,\n",
    "        precision=16, amp_backend=\"native\", gpus=1\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loaders\n",
    "    # ====================================================\n",
    "    train_dataset = TrainDataset(CFG, df_train_fold, input_col='text_augmented')\n",
    "    valid_dataset = TrainDataset(CFG, df_val_fold, input_col='text_augmented')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    hparams = dict(cfg=CFG, num_train_steps=int(len(train_loader) * CFG.epochs), fold=-1)\n",
    "    model = CustomModel(**hparams)\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "    del model\n",
    "    model = CustomModel.load_from_checkpoint(checkpoint_callback.best_model_path, **hparams)\n",
    "    trainer.validate(model, valid_loader)\n",
    "    y_hat = trainer.predict(model, valid_loader)\n",
    "\n",
    "    print(f\"BEST MODEL PATH\", checkpoint_callback.best_model_path)\n",
    "    best_models['all'] = checkpoint_callback.best_model_path\n",
    "\n",
    "    df_val['pred'] = np.concatenate(y_hat)\n",
    "    print(f'Spearman corr fold {fold}', get_score(df_val.score, df_val.pred))\n",
    "\n",
    "\n",
    "def fit_fold():\n",
    "    fit_submodel()\n",
    "    display(df_train.loc[:3, df_train.columns[df_train.columns.str.contains('pred_')]])\n",
    "    display(df_val.loc[:3, df_train.columns[df_train.columns.str.contains('pred_')]])\n",
    "\n",
    "    CFG.max_len = 160\n",
    "    fit_final_model()\n",
    "\n",
    "\n",
    "fit_fold()\n",
    "\n",
    "with open('./logs/best_models.pkl', 'wb') as f:\n",
    "    pickle.dump(best_models, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Spearman corr', get_score(df_train.loc[:, 'score'], df_train.loc[:, 'pred']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    precision=16,\n",
    "    amp_backend=\"native\",\n",
    "    gpus=1\n",
    ")\n",
    "\n",
    "df_test['score'] = np.nan\n",
    "test_dataset = TrainDataset(CFG, df_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "df_test['pred'] = 0\n",
    "\n",
    "with open('./logs/best_models.pkl', 'rb') as f:\n",
    "    best_models = pickle.load(f)\n",
    "\n",
    "\n",
    "for fold in CFG.trn_fold:\n",
    "    hparams = dict(\n",
    "        cfg=CFG,\n",
    "        num_train_steps=None,  # Does not matter since this is prediction\n",
    "        deeper_layer_to_train=CFG.deeper_layer_to_train,  # Does not matter since this is prediction\n",
    "        fold=fold\n",
    "    )\n",
    "\n",
    "    checkpoint = best_models[str(fold)]\n",
    "    print(\"checkpoint\", checkpoint)\n",
    "\n",
    "    model = CustomModel.load_from_checkpoint(checkpoint, **hparams)\n",
    "\n",
    "    preds = predictor.predict(model, test_loader)\n",
    "\n",
    "    df_test[f'pred{fold}'] = np.concatenate(preds)\n",
    "    df_test['pred'] = df_test['pred'] + df_test[f'pred{fold}'] * (1 / CFG.n_fold)\n",
    "\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "df_test[['pred0']].hist(bins=40, ax=axs[0], alpha=0.4)\n",
    "df_test[['pred1']].hist(bins=40, ax=axs[0], alpha=0.4)\n",
    "df_test[['pred2']].hist(bins=40, ax=axs[0], alpha=0.4)\n",
    "df_test[['pred3']].hist(bins=40, ax=axs[0], alpha=0.4)\n",
    "df_test[['pred']].hist(bins=40, ax=axs[1], alpha=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission['score'] = df_test['pred']\n",
    "display(submission.head())\n",
    "submission[['id', 'score']].to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PPPM_debertav3_EL.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30329.017674,
   "end_time": "2022-03-22T18:05:22.040486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-22T09:39:53.022812",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}